{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bf8d2f",
   "metadata": {},
   "source": [
    "# <center> **How to use MOABB and how to validate any algorithm** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc6f7f",
   "metadata": {},
   "source": [
    "The Mother of All BCI Benchmarks (MOABB) is a python library that comprehends all free popular Brain-Computer Interface (BCI) EEG datasets to validate any algorithm for BCIs. Brain-Computer Interface are devices that allows to control by using brain signals, for this, is possible to model different algorithms based on brain data to classify theses signals. After modeling, these algorithms must be validated, in order to know if they are able to accurately predict the brain signal. \n",
    "\n",
    "At first, sounds easy to train and validate theses models to classify brain signals, but due to non-stacionary and low Signal-To-Noise (SNR) features of the brain signals recorded by Electroencefalogran (EEG), is pretty difficult to model algorithms that can generalize for different multiple different trials, subjects and sessions. When fresh research about BCI shows the results of an new algorithm that can performn well compared to others, it lacks of generalization between subjects. And when some reasechers tries to use multiple datasets to validate its model, its is very dificult, due to necessity of different *pipelines* to process the data.\n",
    "\n",
    "To solve this issue [1] developed an python library that has many free EEG datasets that be used to validate an new classification model of brain signals. Is this tutorial will shown how to use this library to validate your own algorithm to classify brain signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08ba09",
   "metadata": {},
   "source": [
    "## **1. Installing MOABB**\n",
    "\n",
    "To install this library is necessary to have Python version 3.8+ already installed on your machine `python==3.8+`, and Pip package installer. With this prerequisite satified, execute following command on your terminal: `pip install moabb`. It is also possible to execute command on the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install moabb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35674ad4",
   "metadata": {},
   "source": [
    "Installing MOABB will automatically install necessary libraries to manipulate data, such as `MNE` and `numpy`. If you and to Guarrantee that all libraries were installed you can also execute the previous command for `numpy` and `mne`, just need to basically: `pip install <package>`, were `<package>` if the name of the library you want to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install mne numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af8696",
   "metadata": {},
   "source": [
    "## **2. Importing Libraries**\n",
    "\n",
    "After installing necessary, we must import them to be usable, using `import`. In our case will be using `import moabb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02da1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy\n",
    "import moabb\n",
    "\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e17b63",
   "metadata": {},
   "source": [
    "## **3. Datasets available**\n",
    "\n",
    "The datasets available in this library are separeted by EEG paradigms, such as **Imagery**, **P300**, **State Steady Visual Evoked Potentials (SSVEP)**, **CVEP** and **Resting State**. It is possible to visualize all dataset available by printing `moabb.datasets.utils.dataset_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed001f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all dataset\n",
    "all_datasets = copy.copy(moabb.datasets.utils.dataset_list) # Getting a copy for data safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74abc164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 77\n",
      "[<class 'moabb.datasets.alex_mi.AlexMI'>, <class 'moabb.datasets.braininvaders.BI2012'>, <class 'moabb.datasets.braininvaders.BI2013a'>, <class 'moabb.datasets.braininvaders.BI2014a'>, <class 'moabb.datasets.braininvaders.BI2014b'>, <class 'moabb.datasets.braininvaders.BI2015a'>, <class 'moabb.datasets.braininvaders.BI2015b'>, <class 'moabb.datasets.bnci.BNCI2014001'>, <class 'moabb.datasets.bnci.BNCI2014002'>, <class 'moabb.datasets.bnci.BNCI2014004'>, <class 'moabb.datasets.bnci.BNCI2014008'>, <class 'moabb.datasets.bnci.BNCI2014009'>, <class 'moabb.datasets.bnci.BNCI2014_001'>, <class 'moabb.datasets.bnci.BNCI2014_002'>, <class 'moabb.datasets.bnci.BNCI2014_004'>, <class 'moabb.datasets.bnci.BNCI2014_008'>, <class 'moabb.datasets.bnci.BNCI2014_009'>, <class 'moabb.datasets.bnci.BNCI2015001'>, <class 'moabb.datasets.bnci.BNCI2015003'>, <class 'moabb.datasets.bnci.BNCI2015004'>, <class 'moabb.datasets.bnci.BNCI2015_001'>, <class 'moabb.datasets.bnci.BNCI2015_003'>, <class 'moabb.datasets.bnci.BNCI2015_004'>, <class 'moabb.datasets.castillos2023.CastillosBurstVEP100'>, <class 'moabb.datasets.castillos2023.CastillosBurstVEP40'>, <class 'moabb.datasets.castillos2023.CastillosCVEP100'>, <class 'moabb.datasets.castillos2023.CastillosCVEP40'>, <class 'moabb.datasets.phmd_ml.Cattan2019_PHMD'>, <class 'moabb.datasets.braininvaders.Cattan2019_VR'>, <class 'moabb.datasets.gigadb.Cho2017'>, <class 'moabb.datasets.neiry.DemonsP300'>, <class 'moabb.datasets.epfl.EPFLP300'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_ERN'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_LRP'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_MMN'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_N170'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_N2pc'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_N400'>, <class 'moabb.datasets.erpcore2021.ErpCore2021_P3'>, <class 'moabb.datasets.fake.FakeDataset'>, <class 'moabb.datasets.fake.FakeVirtualRealityDataset'>, <class 'moabb.datasets.mpi_mi.GrosseWentrup2009'>, <class 'moabb.datasets.phmd_ml.HeadMountedDisplay'>, <class 'moabb.datasets.hinss2021.Hinss2021'>, <class 'moabb.datasets.huebner_llp.Huebner2017'>, <class 'moabb.datasets.huebner_llp.Huebner2018'>, <class 'moabb.datasets.ssvep_exo.Kalunga2016'>, <class 'moabb.datasets.Lee2019.Lee2019_ERP'>, <class 'moabb.datasets.Lee2019.Lee2019_MI'>, <class 'moabb.datasets.Lee2019.Lee2019_SSVEP'>, <class 'moabb.datasets.liu2024.Liu2024'>, <class 'moabb.datasets.ssvep_mamem.MAMEM1'>, <class 'moabb.datasets.ssvep_mamem.MAMEM2'>, <class 'moabb.datasets.ssvep_mamem.MAMEM3'>, <class 'moabb.datasets.mpi_mi.MunichMI'>, <class 'moabb.datasets.ssvep_nakanishi.Nakanishi2015'>, <class 'moabb.datasets.upper_limb.Ofner2017'>, <class 'moabb.datasets.physionet_mi.PhysionetMI'>, <class 'moabb.datasets.alphawaves.Rodrigues2017'>, <class 'moabb.datasets.ssvep_exo.SSVEPExo'>, <class 'moabb.datasets.schirrmeister2017.Schirrmeister2017'>, <class 'moabb.datasets.bbci_eeg_fnirs.Shin2017A'>, <class 'moabb.datasets.bbci_eeg_fnirs.Shin2017B'>, <class 'moabb.datasets.sosulski2019.Sosulski2019'>, <class 'moabb.datasets.stieger2021.Stieger2021'>, <class 'moabb.datasets.thielen2015.Thielen2015'>, <class 'moabb.datasets.thielen2021.Thielen2021'>, <class 'moabb.datasets.braininvaders.VirtualReality'>, <class 'moabb.datasets.ssvep_wang.Wang2016'>, <class 'moabb.datasets.Weibo2014.Weibo2014'>, <class 'moabb.datasets.Zhou2016.Zhou2016'>, <class 'moabb.datasets.braininvaders.bi2012'>, <class 'moabb.datasets.braininvaders.bi2013a'>, <class 'moabb.datasets.braininvaders.bi2014a'>, <class 'moabb.datasets.braininvaders.bi2014b'>, <class 'moabb.datasets.braininvaders.bi2015a'>, <class 'moabb.datasets.braininvaders.bi2015b'>]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of datasets: {len(all_datasets)}\")\n",
    "print(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541ad399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AlexMI'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets[0].__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97817c",
   "metadata": {},
   "source": [
    "It is also possible to select these datasets by its paradigm, it is possible to do this by using `moabb.datasets.utils.dataset_search()` function, where we can pass some parameters to select the perfect dataset we want, one of these parameters is `paradigm`, which can be `'imagery'`, `'p300'`, `'ssvep'` and `'cvep'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76cef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting SSVEP paradigm\n",
    "ssvep_datasets = copy.deepcopy(moabb.datasets.utils.dataset_search(paradigm='ssvep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of SSVEP datasets: {len(ssvep_datasets)}\")\n",
    "print(ssvep_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f0789",
   "metadata": {},
   "source": [
    "In the function `moabb.datasets.utils.dataset_search()` there are other parameters to filter, such as: \n",
    "* `multi_session` - Returns just the datasets that that has more than one session per subject.\n",
    "* `events` - Type of event to select.\n",
    "* `has_all_events` - Select dataset with all types of events.\n",
    "* `interval` - (Motor Imagery Only) minimal time length of the event.\n",
    "* `min_subjects` - Minum number of subjects in an dataset.\n",
    "* `channels` - List of channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94753c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moabb.datasets.utils.dataset_search(paradigm='imagery',\n",
    "                                    multi_session=True,\n",
    "                                    min_subjects=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf757d",
   "metadata": {},
   "source": [
    "It also possible to create fake dataset using Python library `fake` [2]. When used the functions used above, it shows class `moabb.datasets.fake.FakeDataset()`, which is a class implemented for test purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = moabb.datasets.fake.FakeDataset(event_list=['fake1', 'fake2'],\n",
    "                                            n_sessions=2,\n",
    "                                            n_runs=2,\n",
    "                                            paradigm='imagery',\n",
    "                                            channels=('C3', 'Cz', 'C4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2dacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fake_data.get_data()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a2b6a",
   "metadata": {},
   "source": [
    "## **4. Main Concepts**\n",
    "\n",
    "Before diving into how to use these datasets to create an pipeline, it is important to know the four main concepts of the MOABB: (1) Datasets, (2) Paradigms, (3) Evaluation, and (4) Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e127f4d",
   "metadata": {},
   "source": [
    "### **4.1 Datasets**\n",
    "\n",
    "Since we already know how to seach and select desired datasets, we'll start selecting dataset for `imagery` paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fe6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all datasets with motor imagery\n",
    "imagery_datasets = copy.copy(moabb.datasets.utils.dataset_search(paradigm='imagery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e84d7a",
   "metadata": {},
   "source": [
    "It is possible to get the data by using `.get_data()` function. Before using this function, which will download each dataset, is important to set the path directory, by using `moabb.utils.set_downalod_dir(path)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting download path. Sometimes this function does not work\n",
    "moabb.utils.set_download_dir(path='./datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efaf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading each dataset\n",
    "#[dataset.get_data() for dataset in imagery_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefe53d",
   "metadata": {},
   "source": [
    "### **4.2 Paradigms**\n",
    "\n",
    "As we explained before, there are four paradigms, which are: (1) Motor Imagery, (2) SSVEP, (3) P300, (4) CVEP, and (5) Resting State. Each of theses paradigms defines how raw MNE data will be processed and feed to the decoing algorithm.\n",
    "\n",
    "For Motor imagery paradigm:\n",
    "1. `MotorImagery()` - N Classes, N is the number of classes desired.\n",
    "2. `LeftRightImagery()`\n",
    "3. `FilterBankLeftRightImagery()`\n",
    "4. `FilterBankMotorImagery()`\n",
    "\n",
    "For P300 paradigm:\n",
    "1. `SinglePass()`\n",
    "2. `P300()`\n",
    "\n",
    "For SSVEP paradigm:\n",
    "1. `SSVEP()`\n",
    "2. `FilterBankSSVEP`\n",
    "\n",
    "For c-VEP Paradigms:\n",
    "1. `CVEP()`\n",
    "2. `FilterBankCVEP()`\n",
    "\n",
    "For Resting State Paradigms:\n",
    "1. `RestingStateToP300Adapter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0edc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LeftRight for MotorImagery\n",
    "paradigm = moabb.paradigms.LeftRightImagery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8b29d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<moabb.datasets.bnci.BNCI2014_001 object at 0x000001C61B01C340>, <moabb.datasets.bnci.BNCI2014_004 object at 0x000001C61B01FE80>, <moabb.datasets.gigadb.Cho2017 object at 0x000001C61B01F160>, <moabb.datasets.mpi_mi.GrosseWentrup2009 object at 0x000001C61B01C0A0>, <moabb.datasets.Lee2019.Lee2019_MI object at 0x000001C61B01C1F0>, <moabb.datasets.liu2024.Liu2024 object at 0x000001C61B01DE10>, <moabb.datasets.physionet_mi.PhysionetMI object at 0x000001C61B01CFA0>, <moabb.datasets.schirrmeister2017.Schirrmeister2017 object at 0x000001C61B01FF70>, <moabb.datasets.bbci_eeg_fnirs.Shin2017A object at 0x000001C61B01C8B0>, <moabb.datasets.stieger2021.Stieger2021 object at 0x000001C61B01CA30>, <moabb.datasets.Weibo2014.Weibo2014 object at 0x000001C61B035390>, <moabb.datasets.Zhou2016.Zhou2016 object at 0x000001C61B0367A0>]\n"
     ]
    }
   ],
   "source": [
    "print(paradigm.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4e78b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<moabb.datasets.gigadb.Cho2017 at 0x1c61b036950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagery_datasets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0292711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\datasets\\download.py:56: RuntimeWarning: Setting non-standard config type: \"MNE_DATASETS_GIGADB_PATH\"\n",
      "  set_config(key, get_config(\"MNE_DATA\"))\n",
      "Downloading data from 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100295/mat_data/s01.mat' to file 'C:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\notebooks\\.5524\\100001_101000\\100295\\mat_data\\s01.mat'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3.ap-northeast-1.wasabisys.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "  7%|##7                                   | 14.9M/203M [00:03<00:27, 6.92MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, labels, meta \u001b[38;5;241m=\u001b[39m \u001b[43mparadigm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagery_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\paradigms\\base.py:274\u001b[0m, in \u001b[0;36mBaseProcessing.get_data\u001b[1;34m(self, dataset, subjects, return_epochs, return_raws, cache_config, postprocess_pipeline)\u001b[0m\n\u001b[0;32m    269\u001b[0m process_pipelines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    270\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    271\u001b[0m )\n\u001b[0;32m    272\u001b[0m labels_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[1;32m--> 274\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    275\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mget_data(\n\u001b[0;32m    276\u001b[0m         subjects\u001b[38;5;241m=\u001b[39msubjects,\n\u001b[0;32m    277\u001b[0m         cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[0;32m    278\u001b[0m         process_pipeline\u001b[38;5;241m=\u001b[39mprocess_pipeline,\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m process_pipeline \u001b[38;5;129;01min\u001b[39;00m process_pipelines\n\u001b[0;32m    281\u001b[0m ]\n\u001b[0;32m    283\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    284\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\paradigms\\base.py:275\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    269\u001b[0m process_pipelines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    270\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    271\u001b[0m )\n\u001b[0;32m    272\u001b[0m labels_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[0;32m    274\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 275\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m process_pipeline \u001b[38;5;129;01min\u001b[39;00m process_pipelines\n\u001b[0;32m    281\u001b[0m ]\n\u001b[0;32m    283\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    284\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\datasets\\base.py:433\u001b[0m, in \u001b[0;36mBaseDataset.get_data\u001b[1;34m(self, subjects, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subject \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_list:\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid subject \u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(subject))\n\u001b[1;32m--> 433\u001b[0m     data[subject] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_single_subject_data_using_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m check_subject_names(data)\n\u001b[0;32m    439\u001b[0m check_session_names(data)\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\datasets\\base.py:527\u001b[0m, in \u001b[0;36mBaseDataset._get_single_subject_data_using_cache\u001b[1;34m(self, subject, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;66;03m# Load and eventually overwrite:\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cached_steps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# last option: we don't use cache\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m     sessions_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_single_subject_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sessions_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# should not happen\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\datasets\\gigadb.py:71\u001b[0m, in \u001b[0;36mCho2017._get_single_subject_data\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_single_subject_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, subject):\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return data for a single subject.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     data \u001b[38;5;241m=\u001b[39m loadmat(\n\u001b[0;32m     74\u001b[0m         fname,\n\u001b[0;32m     75\u001b[0m         squeeze_me\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     76\u001b[0m         struct_as_record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m         verify_compressed_data_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     78\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\datasets\\gigadb.py:125\u001b[0m, in \u001b[0;36mCho2017.data_path\u001b[1;34m(self, subject, path, force_update, update_path, verbose)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid subject number\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    124\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;132;01m{:02d}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(GIGA_URL, subject)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGIGADB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<decorator-gen-464>:12\u001b[0m, in \u001b[0;36mdata_dl\u001b[1;34m(url, sign, path, force_update, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\moabb\\datasets\\download.py:156\u001b[0m, in \u001b[0;36mdata_dl\u001b[1;34m(url, sign, path, force_update, verbose)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     known_hash \u001b[38;5;241m=\u001b[39m file_hash(\u001b[38;5;28mstr\u001b[39m(destination))\n\u001b[1;32m--> 156\u001b[0m dlpath \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknown_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dlpath\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\pooch\\core.py:239\u001b[0m, in \u001b[0;36mretrieve\u001b[1;34m(url, known_hash, fname, path, processor, downloader, progressbar)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m downloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     downloader \u001b[38;5;241m=\u001b[39m choose_downloader(url, progressbar\u001b[38;5;241m=\u001b[39mprogressbar)\n\u001b[1;32m--> 239\u001b[0m \u001b[43mstream_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m known_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     get_logger()\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHA256 hash of downloaded file: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse this value as the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknown_hash\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooch.retrieve\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m         file_hash(\u001b[38;5;28mstr\u001b[39m(full_path)),\n\u001b[0;32m    248\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\pooch\\core.py:807\u001b[0m, in \u001b[0;36mstream_download\u001b[1;34m(url, fname, known_hash, downloader, pooch, retry_if_failed)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;66;03m# Stream the file to a temporary so that we can safely check its\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;66;03m# hash before overwriting the original.\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_file(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(fname\u001b[38;5;241m.\u001b[39mparent)) \u001b[38;5;28;01mas\u001b[39;00m tmp:\n\u001b[1;32m--> 807\u001b[0m         \u001b[43mdownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m         hash_matches(tmp, known_hash, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(fname\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m    809\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mmove(tmp, \u001b[38;5;28mstr\u001b[39m(fname))\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\pooch\\downloaders.py:240\u001b[0m, in \u001b[0;36mHTTPDownloader.__call__\u001b[1;34m(self, url, output_file, pooch, check_only)\u001b[0m\n\u001b[0;32m    238\u001b[0m     progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogressbar\n\u001b[0;32m    239\u001b[0m     progress\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m=\u001b[39m total\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[0;32m    242\u001b[0m         output_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erika\\Documents\\Code\\eeg-pyriemann-pipeline\\.venv\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|##8                                   | 15.3M/203M [00:19<00:27, 6.92MB/s]"
     ]
    }
   ],
   "source": [
    "X, labels, meta = paradigm.get_data(dataset=imagery_datasets[6], subjects=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b079c0",
   "metadata": {},
   "source": [
    "### **4.3 Pipeline**\n",
    "\n",
    "A pipeline in ML/DATA engineering refers to the end-to-end sequence of steps that data undergoes—from raw input to model predictions (or insights). Is this case we will be using for model training. We will me using scikit function `make_pipeline` for this purpouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(CSP(n_components=8), LDA())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bb0ce",
   "metadata": {},
   "source": [
    "### **4.4 Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = moabb.evaluations.WithinSessionEvaluation(\n",
    "    paradigm=paradigm,\n",
    "    datasets=[imagery_datasets[0]],\n",
    "    overwrite=True,\n",
    "    hdf5_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64398ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation.process({\"csp+lda\": pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c831d",
   "metadata": {},
   "source": [
    "### **4.5 Statistics, visualization and utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02294",
   "metadata": {},
   "outputs": [],
   "source": [
    "moabb.analysis.plotting.score_plot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd46c1b",
   "metadata": {},
   "source": [
    "## **References**\n",
    "\n",
    "* [1] MOABB - https://moabb.neurotechx.com/docs/api.html\n",
    "* [2] Faker - https://faker-readthedocs-io.translate.goog/en/master/?_x_tr_sl=en&_x_tr_tl=pt&_x_tr_hl=pt&_x_tr_pto=tc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
